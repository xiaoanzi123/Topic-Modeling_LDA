{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# packages to store and manipulate data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# model building package\n",
    "import sklearn\n",
    "\n",
    "# package to clean text\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenanfan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print n_top_words\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/chenanfan/Aaron_Topic_model/clean_tweet.xlsx\", index_col=None,na_values=['NA'])\n",
    "df.shape\n",
    "n_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 44489\n",
      "INFO:lda:vocab_size: 5000\n",
      "INFO:lda:n_words: 590228\n",
      "INFO:lda:n_topics: 15\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "/Users/chenanfan/anaconda3/lib/python3.6/site-packages/lda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:lda:<0> log likelihood: -6616531\n",
      "INFO:lda:<10> log likelihood: -4317095\n",
      "INFO:lda:<20> log likelihood: -4042364\n",
      "INFO:lda:<30> log likelihood: -3980212\n",
      "INFO:lda:<40> log likelihood: -3949851\n",
      "INFO:lda:<50> log likelihood: -3927702\n",
      "INFO:lda:<60> log likelihood: -3914070\n",
      "INFO:lda:<70> log likelihood: -3907214\n",
      "INFO:lda:<80> log likelihood: -3895984\n",
      "INFO:lda:<90> log likelihood: -3889617\n",
      "INFO:lda:<99> log likelihood: -3885366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(lowercase=False,\n",
    "                                strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.content)\n",
    "vocab=tf_vectorizer.get_feature_names()\n",
    "model = lda.LDA(n_topics=15, n_iter=100, random_state=1)  \n",
    "model.fit(tf)\n",
    "print('model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (15, 5000)\n",
      "['aabpbpzu', 'aan', 'aaya']\n",
      "[[5.82954413e-07 5.82954413e-07 5.82954413e-07]\n",
      " [2.17822214e-07 2.17822214e-07 2.17822214e-07]\n",
      " [1.91673695e-07 1.91673695e-07 1.91673695e-07]\n",
      " [1.66378278e-07 1.66378278e-07 1.66378278e-07]\n",
      " [2.75451741e-07 2.75451741e-07 2.75451741e-07]\n",
      " [1.64209005e-07 1.64209005e-07 1.64209005e-07]\n",
      " [9.36140930e-04 2.75254611e-07 2.75254611e-07]\n",
      " [4.18532625e-07 4.18532625e-07 4.18532625e-07]\n",
      " [2.75687150e-07 2.75687150e-07 2.75687150e-07]\n",
      " [2.88001843e-07 2.88001843e-07 2.88001843e-07]\n",
      " [3.20523094e-07 3.20523094e-07 3.20523094e-07]\n",
      " [2.12318733e-07 2.12318733e-07 3.61154165e-04]\n",
      " [2.24789822e-07 2.24789822e-07 2.24789822e-07]\n",
      " [3.39190014e-07 1.18750424e-03 3.39190014e-07]\n",
      " [2.86098475e-07 2.86098475e-07 2.86098475e-07]]\n",
      "topic: 0 sum: 0.9999999999999971\n",
      "topic: 1 sum: 0.9999999999999298\n",
      "topic: 2 sum: 0.9999999999999679\n",
      "topic: 3 sum: 0.9999999999998656\n",
      "topic: 4 sum: 1.0000000000000224\n",
      "topic: 5 sum: 0.9999999999999072\n",
      "topic: 6 sum: 1.0000000000000553\n",
      "topic: 7 sum: 1.0000000000001106\n",
      "topic: 8 sum: 0.9999999999999925\n",
      "topic: 9 sum: 1.0000000000001215\n",
      "topic: 10 sum: 1.0000000000000837\n",
      "topic: 11 sum: 1.000000000000075\n",
      "topic: 12 sum: 1.0000000000000537\n",
      "topic: 13 sum: 1.0000000000001168\n",
      "topic: 14 sum: 1.0000000000000226\n"
     ]
    }
   ],
   "source": [
    "#distribution of topic-word\n",
    "topic_word = model.topic_word_ \n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    "print(vocab[:3])\n",
    "print(topic_word[:, :3])\n",
    "for n in range(15):\n",
    "    sum_pr = sum(topic_word[n,:])  \n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Topic 0\n",
      "- ไวร coronavirus ncov นธ ใหม สโคโรน สโคโรนาสายพ สโคโรนา กร wuhan\n",
      "*Topic 1\n",
      "- coronaviru china coronavirus travel spread medic wuhan outbreak chines hong\n",
      "*Topic 2\n",
      "- health emerg declar public intern concern time coronavirus global com\n",
      "*Topic 3\n",
      "- china wuhan coronaviru death ncov coronavirus case confirm report outsid\n",
      "*Topic 4\n",
      "- travel china pheic coronavirus ncov coronavirusaustralia countri ban declar restrict\n",
      "*Topic 5\n",
      "- coronavirus wuhan wuhancoronavirus viru china outbreak peopl pneumonia coronaviru virus\n",
      "*Topic 6\n",
      "- ncov coronavirus twitter pic com inform transmiss novel coronaviru infect\n",
      "*Topic 7\n",
      "- le coronaviru coronavirus la wuhan twitter pic com en chine\n",
      "*Topic 8\n",
      "- coronaviru wuhan coronavirus china like look taiwan govern market virolog\n",
      "*Topic 9\n",
      "- wuhan coronavirus china india die evacu case student der indian\n",
      "*Topic 10\n",
      "- coronaviru zhong nanshan expert china day sar respiratori say wuhan\n",
      "*Topic 11\n",
      "- twitter pic com china coronaviru coronavirus viru flu wuhan outbreak\n",
      "*Topic 12\n",
      "- la el en coronaviru que coronavirus china del por lo\n",
      "*Topic 13\n",
      "- wuhan di coronavirus coronaviru twitter com viru pic il china\n",
      "*Topic 14\n",
      "- coronavirus wuhancoronavirus coronavirusoutbreak wuhan ncov 新型肺炎 china chinacoronavirus 新型コロナウイルス wuhanvirus\n"
     ]
    }
   ],
   "source": [
    "#computing Top-N words of each topic\n",
    "import numpy as np\n",
    "n = 10\n",
    "for i, topic_dist in enumerate(topic_word):  \n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]  \n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(doc_topic): <class 'numpy.ndarray'>\n",
      "shape: (44489, 15)\n"
     ]
    }
   ],
   "source": [
    "#Document-Topic distribution\n",
    "doc_topic = model.doc_topic_  \n",
    "print(\"type(doc_topic): {}\".format(type(doc_topic)))  \n",
    "print(\"shape: {}\".format(doc_topic.shape))\n",
    "topicList=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#export all the document-tpoic results to a .xlsx file\n",
    "for n in range(len(doc_topic)):  \n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    topicList.append(topic_most_pr)\n",
    "    #print(\"doc: {} topic: {}\".format(n, topic_most_pr))  \n",
    "topicdf=pd.DataFrame(topicList)\n",
    "df['topic']=topicList\n",
    "df.to_excel('Coronavirus-20topics-10words.xlsx',sheet_name='Sheet1')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
